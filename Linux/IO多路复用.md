# I/O 多路复用

- IO多路复用是指内核一旦发现进程指定的一个或者多个IO条件准备读取，它就通知该进程。
- select/poll/epoll 都是 I/O 多路复用的具体实现，select 出现的最早，之后是 poll，再是 epoll。
- 可以让单个进程具有处理多个 I/O 事件的能力。又被称为 Event Driven I/O，即事件驱动 I/O。
- 可以让单个进程处理多个链接
- 如果一个 Web 服务器没有 I/O 复用，那么每一个 Socket 连接都需要创建一个线程去处理。如果同时有几万个连接，那么就需要创建相同数量的线程。相比于多进程和多线程技术，I/O 复用不需要进程线程创建和切换的开销，系统开销更小。

## IO多路复用适用场合

- 1）当客户处理多个描述符时（一般是交互式输入和网络套接口），必须使用I/O复用
- 2）当一个客户同时处理多个套接口时，这种情况是可能的，但很少出现。
- 3）如果一个TCP服务器既要处理监听套接口，又要处理已连接套接口，一般也要用到I/O复用。
- 4）如果一个服务器即要处理TCP，又要处理UDP，一般要使用I/O复用。
- 5）如果一个服务器要处理多个服务或多个协议，一般要使用I/O复用。

## select

- 使用 select 等待数据会轮询所有套接字，当某个套接字有数据到达，就通知用户进程
- select 有三种类型的描述符类型：readset、writeset、exceptset，分别对应读、写、异常条件的描述符集合。
- select具有O(n)的无差别轮询复杂度，同时处理的流越多，无差别轮询时间就越长。

### select缺点

- 单个进程所打开的FD是有一定限制的，它由FD_SETSIZE设置，默认值是1024。一般来说这个数目和系统内存关系很大，具体数目可以cat /proc/sys/fs/file-max察看。32位机默认是1024个。64位机默认是2048.
- 对socket进行扫描时是线性扫描，即采用轮询的方法，效率较低。当套接字比较多的时候，每次select()都要通过遍历FD_SETSIZE个Socket来完成调度，不管哪个Socket是活跃的，都遍历一遍。
- 需要维护一个用来存放大量fd的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大。

## poll

- 时间复杂度O(n)
- poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态
- 但是它没有最大连接数的限制，原因是它是基于链表来存储的.
- poll是LT（水平触发），如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd。

### poll缺点：

- 大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是不是有意义
- 同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。

## select 和 poll 比较

- select 会修改描述符，而 poll 不会；
- select 的描述符类型(fd_set)使用数组实现，FD_SETSIZE 大小默认为 1024，因此默认只能监听 1024 个描述符。如果要监听更多描述符的话，需要修改 FD_SETSIZE 之后重新编译；而 poll 的描述符类型(pollfd)使用链表实现，没有描述符数量的限制；
- poll 提供了更多的事件类型，并且对描述符的重复利用上比 select 高。
- select 和 poll 每次调用都需要将全部描述符从应用进程缓冲区复制到内核缓冲区。
- select 和 poll 的返回结果中没有声明哪些描述符已经准备好，所以如果返回值大于 0 时，应用进程都需要使用轮询的方式来找到 I/O 完成的描述符。
- poll 在应付大文件的时候快
- 几乎所有的系统都支持 select，但是只有比较新的系统支持 poll。

## epoll

- 当套接字比较多的时候，每次select()都要通过遍历FD_SETSIZE个Socket来完成调度,不管哪个Socket是活跃的,都遍历一遍。这会浪费很多CPU时间。
- epoll给套接字注册某个回调函数，当他们活跃时，自动完成相关操作，避免了轮询
- 已注册的描述符在内核中会被维护在一棵红黑树上，通过回调函数内核会将 I/O 准备好的描述符加入到一个链表中管理，进程调用 epoll_wait() 便可以得到事件完成的描述符。
- epoll 只运行在 Linux 平台上，select 可移植性更好，几乎被所有主流平台所支持。
- epoll 可以同时轮询大量的描述符，并且这些连接最好是长连接。如果需要同时监控小于 1000 个描述符，而且都是非常短暂的，就没有必要使用 epoll

## epoll的优点

- 没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）。
- 效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。只有活跃可用的FD才会调用callback函数；只关注“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll。
- 内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。
- epoll 的效率不随打开文件数目的增加而线性下降
- epoll 只需要将描述符从进程缓冲区向内核缓冲区拷贝一次，并且进程不需要通过轮询来获得事件完成的描述符。

## epoll工作模式

- epoll 的描述符事件有两种触发模式：LT（level trigger）（水平触发）和 ET（edge trigger）（边缘触发）。
- LT 模式：当 epoll_wait() 检测到采用LT的描述符事件到达时，将此事件通知进程，进程可以不立即处理该事件，下次调用 epoll_wait() 会再次通知进程。是默认的一种模式，并且同时支持 Blocking 和 No-Blocking。
- ET 模式：当 epoll_wait() 检测到采用ET的描述符事件到达时，将此事件通知进程，只通知一次，所以通知之后进程必须立即处理事件，下次再调用 epoll_wait() 时不会再得到事件到达的通知。
- ET 模式：很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。只支持 No-Blocking，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。

## epoll为什么要有ET触发模式？

- 如果采用LT模式的话，系统中一旦有大量你不需要读写的就绪文件描述符，它们每次调用epoll_wait都会返回，这样会大大降低处理程序检索自己关心的就绪文件描述符的效率
- 而采用ET这种边沿触发模式的话，当被监控的文件描述符上有可读写事件发生时，epoll_wait()会通知处理程序去读写。如果这次没有把数据全部读写完(如读写缓冲区太小)，那么下次调用epoll_wait()时，它不会通知你，也就是它只会通知你一次，直到该文件描述符上出现第二次可读写事件才会通知你，这种模式比水平触发效率高，系统不会充斥大量你不关心的就绪文件描述符

## epoll 为什么速度快

- 调用epoll_ctl往里塞入百万个句柄时，epoll_wait仍然可以飞快的返回，并有效的将发生事件的句柄给我们用户。
- 这是由于我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件，当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。而不需要像select那样需要在用户态去判断每个套接字上是否有事件发生。所以，epoll_wait非常高效。

## epoll 工作流程

- 某个进程调用执行epoll_create()时，会创建红黑树和就绪链表，已注册的描述符在内核中会被维护在一棵红黑树上，通过回调函数内核会将 I/O 准备好的描述符加入到准备就绪list链表中管理。
- epoll_ctl()用于向内核注册新的描述符或者是改变某个文件描述符的状态，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据。
- 执行epoll_wait()时立刻返回准备就绪链表里的数据即可。

## 不同的线程或者进程在处理同一个SOCKET的事件

- 如果是多线程在处理，一个SOCKET事件到来，数据开始解析，这时候这个SOCKET又来了同样一个这样的事件，而你的数据解析尚未完成，那么程序会自动调度另外一个线程或者进程来处理新的事件，这造成一个很严重的问题，不同的线程或者进程在处理同一个SOCKET的事件，这会使程序的健壮性大降低而编程的复杂度大大增加，即使在ET模式下也有可能出现这种情况

解决方法：

- 第一种方法是在单独的线程或进程里解析数据，也就是说，接收数据的线程接收到数据后立刻将数据转移至另外的线程。

- 第二种方法 EPOLL_ONESHOT 方法，如果对描述符socket注册了 EPOLL_ONESHOT 事件，那么操作系统最多触发其上注册的一个可读、可写或者异常事件，且只触发一次。想要下次再触发则必须使用epoll_ctl重置该描述符上注册的事件，包括 EPOLL_ONESHOT 事件。